# Copyright 2022 Tom SF Haines

import sys

import numpy

try:
  import pyximport
  pyximport.install(setup_args={'include_dirs': [numpy.get_include()]})
except:
  pass

try:
  from . import regorofun
except ImportError:
  import regorofun



class RegOrogram:
  """An orogram with regularly spaced bins - primarily for incremental data collection as regular spacing makes that easy. Has an automatically expanding range, based on a dictionary of blocks, so the only parameter of consequence is the spacing between adjacent bins. Typical usage is to collect data in this with a very fine spacing then simplify to an Orogram with variable spacing that is data driven. Functionality is still pretty rich, just incase you want to set a data-suitable bin width and use it as a direct density estimate."""
  __slots__ = ('_spacing', '_blocksize', '_data', '_total', '_low', '_high', '_cdf')
  
  
  def __init__(self, spacing = 0.001, blocksize = 1024 * 16):
    """spacing is how wide the bins are, as in bin center to bin center; blocksize is how many bin values to record in each block of the backing storage (you can usually leave this alone)."""
    self._spacing = spacing
    self._blocksize = blocksize
    
    self._data = dict() # block # -> float numpy array of counts
    
    self._total = 0.0 # Total weight captured
    self._low = sys.maxsize # Index of lowest nonzero bin
    self._high = -sys.maxsize - 1 # Index of highest nonzero bin
    
    self._cdf = None # None if not calculated, otherwise an array indexed by bin starting with self._low - 1 at [0]
  
  
  def copy(self):
    """Makes a copy."""
    ret = RegOrogram(self._spacing, self._blocksize)
    
    for k, v in self._data.items():
      ret._data[k] = v.copy()
    
    ret._total = self._total
    ret._low = self._low
    ret._high = self._high
    
    ret._cdf = self._cdf # Never edited so safe
    
    return ret
  
  __copy__ = copy
  
  
  def __sizeof__(self):
    ret = super().__sizeof__()
    
    ret += sys.getsizeof(self._spacing)
    ret += sys.getsizeof(self._blocksize)
    ret += sys.getsizeof(self._blocksize)
    ret += sys.getsizeof(self._data)
    ret += sys.getsizeof(self._total)
    ret += sys.getsizeof(self._low)
    ret += sys.getsizeof(self._high)
    ret += sys.getsizeof(self._cdf)
    
    for k, v in self._data.items():
      ret += sys.getsizeof(k)
      ret += sys.getsizeof(v)
    
    return ret
  
  
  def __len__(self):
    """Returns a length that can be seen as the number of parameters, in the sense of being the number of values needed to define the heights for the range that is used. It includes the zeros either side in that count. Note what this excludes: start position and run length of each range."""
    return regorofun.length(self._data, self._blocksize, self._low, self._high)
  
  
  def spacing(self):
    """Returns the spacing this object was constructed with."""
    return self._spacing
  
  
  def blocksize(self):
    """Returns the blocksize this object was constructed with."""
    return self._blocksize
  
  
  def add(self, x, weight = 1.0):
    """Adds the set of samples x (single value or array) to the orogram. Can optionally include a weight, to count each sample as being some multiple of one (single value or array; broadcast as required)."""
    x = numpy.atleast_1d(x)
    weight = numpy.broadcast_to(weight, x.shape).astype(numpy.float32)
    
    x = x / self._spacing
    base = numpy.floor(x).astype(int)
    t = (x - base).astype(numpy.float32)
    
    regorofun.add(self._data, self._blocksize, base, t, weight)
    
    self._total += weight.sum()
    self._low = min(self._low, base.min())
    self._high = max(self._high, base.max() + 1)
    
    self._cdf = None
  
  
  def binadd(self, base, density, total = None):
    """Lets you add values directly to the bins, where those values can be seen as the number of samples worth of mass. The add function does the same thing if you give it weighted samples (weight=density) at the exact bin centers, but this is faster. base is the bin index of density[0]. You can optionally provide total, the sum of the density array if you know it."""
    regorofun.binadd(self._data, self._blocksize, base, density.astype(numpy.float32))
    
    if total is None:
      total = density.sum()
    
    self._total += total
    self._low = min(self._low, base)
    self._high = max(self._high, base + density.shape[0] - 1)
    
    self._cdf = None
  
  
  def bake(self, cdf, start, end, weight = 1, vectorised = True):
    """Lets you 'bake' an arbitrary distribution into this representation. You have to provide a function for evaluating the CDF of that distribution, plus the range to evaluate (it ensures the mass in the range sums to 1). By default it aweightssumes that the CDF functon is vectorised but you can indicate it is not (vectorised parameter); that will be slow however. Solves the linear equation so that the areas between the bin centers match the mass of the CDF. Note that baking is addative, so the values will be summed on top of anything already within the model, by default as though this is a single samples worth of evidence, but the 'weight' parameter lets you change that to whatever they want."""
    
    # Calculate bin range...
    low = int(numpy.floor(start / self._spacing))
    high = int(numpy.ceil(end / self._spacing))
    
    # Generate array of relevant bin centers...
    centers = numpy.arange(low, high+1, dtype=numpy.float32)
    centers *= self._spacing
    
    # Evaluate cdf at each bin center...
    if vectorised:
      cummass = cdf(centers)
      
    else:
      cummass = numpy.empty(centers.shape, dtype=numpy.float32)
      for i in range(cummass.shape[0]):
        cummass[i] = cdf(centers[i])
    
    # Convert to mass between bin centers, renormalise...
    between = (cummass[1:] - cummass[:-1]).astype(numpy.float32)
    between /= between.sum()
    
    # Solve for bin center quantities...
    density = numpy.empty(cummass.shape, dtype=numpy.float32)
    regorofun.mass2weight(between, density)
    
    # Scale by weight...
    density *= weight
    
    # Update data structure by adding in these values...
    self.binadd(low, density, weight)
  
  
  def sum(self):
    """Returns the sum of all the bins, in effect the total weight recorded. Note that it's a float because fractional weights can be added (plus weight is distributed between bin centers)."""
    return self._total
  
  
  def min(self):
    """Returns the minimum value to have any weight assigned to it. Note that the probability at this point will be zero, but nonzero if you move up by some epsilon."""
    return (self._low - 1) * self._spacing
  
  
  def max(self):
    """Returns the maximum value to have any weight assigned to it. Note that the probability at this point will be zero, but nonzero if you move down by some epsilon."""
    return (self._high + 1) * self._spacing


  def binmin(self):
    """Returns the lowest index of a bin that contains weight minus one. The minus one means you have the same bin that min generates and it fully covers the range that actually contains probability mass."""
    return self._low - 1
  
  
  def binmax(self):
    """Returns the highest index of a bin that contains weight plus one. The plus one means you have the same bin that max generates and it fully covers the range that actually contains probability mass."""
    return self._high + 1


  def center(self, i):
    """Given the index of a bin converts it to the position of the centre of the bin. Obviously just a multiplication, but abstracted for consistency and to avoid mistakes. Vectorised."""
    return numpy.asarray(i, dtype=numpy.float32) * self._spacing


  def weight(self, i):
    """Given the index of a bin (or many - vectorised) returns how much weight has landed in that bin. This is sort of the number of samples that have landed in that bin, noting that because it's triangular each sample gets split between adjacent bins."""
    if numpy.ndim(i)==0:
      block = i // self._blocksize
      if block in self._data:
        return self._data[block][i - block*self._blocksize]
      else:
        return 0.0
    
    else:
      i = numpy.asarray(i, dtype=int)
      ret = numpy.empty(i.shape, dtype=numpy.float32)
      regorofun.binweight(self._data, self._blocksize, i, ret)
      return ret
  
  
  def prob(self, i):
    """Given the index of a bin (or many - vectorised) returns the probability at the center of that bin. Note that this is a pdf, not a pmf, and you need linear interpolation to get between bins (which is what you get for arbitrary values if you call this object)."""
    if numpy.ndim(i)==0:
      block = i // self._blocksize
      if block in self._data:
        return self._data[block][i - block*self._blocksize] / (self._spacing * self._total)
      else:
        return 0.0
    
    else:
      i = numpy.asarray(i, dtype=int)
      ret = numpy.empty(i.shape, dtype=numpy.float32)
      regorofun.binweight(self._data, self._blocksize, i, ret)
      ret /= self._spacing * self._total
      return ret
  
  
  def __call__(self, x):
    """Evaluates the probability at the given x, including linear interpolation between bin centers. Vectorised."""
    x = numpy.asarray(x) / self._spacing
    base = numpy.floor(x).astype(int)
    t = x - base
    
    if numpy.ndim(x)==0:
      ret = 0.0
      
      block1 = base // self._blocksize
      block1i = base - block*self._blocksize
      
      if block1 in self._data:
        dic = self._data[block1]
        ret += (1 - t) * dic[block1i]
        if block1i+1 < self._blocksize:
          ret += t * dic[block1i+1]
          
      elif (block1i+1 == self._blocksize) and (block1+1 in self._data):
        ret += t * self._data[block1+1][0]
      
      return ret / (self._spacing * self._total)
    
    else:
      ret = numpy.zeros(base.shape, dtype=numpy.float32)
      regorofun.weight(self._data, self._blocksize, base, t.astype(numpy.float32), ret)
      ret /= self._spacing * self._total
      return ret
  
  
  def modes(self):
    """Returns an array containing the position of every mode. Pretty useless under default use, but becomes useful if bin spacing is made large enough to regularise."""
    ret = regorofun.modes(self._data, self._blocksize, self._low, self._high)
    return ret.astype(numpy.float32) * self._spacing
  
  
  def binmodes(self):
    """Identical to modes() but returns the bin indices instead."""
    return regorofun.modes(self._data, self._blocksize, self._low, self._high)

  
  def highest(self):
    """Returns the position with the highest probability."""
    return self.binhighest() * self._spacing

  
  def binhighest(self):
    """Returns the bin with the highest probability."""
    best = self._low
    weight = 0.0
    
    for k, v in self._data.items():
      localbest = numpy.argmax(v)
      localweight = v[localbest]
      
      if localweight > weight:
        best = localbest + k*self._blocksize
        weight = localweight
    
    return best
  
  
  def graph(self, start = None, end = None):
    """Returns two aligned vectors, as a conveniance function for generating the arrays to hand to a graph plotting function. Return is a tuple of (x, y), x being the bin center and y the probability. You can give the start and end bin if you want (inclusive, exclusive), but it defaults to the range of the observed data (going one wider so the end points have probability zero)."""
    if start is None:
      start = self._low - 1
    
    if end is None:
      end = self._high + 2
    
    i = numpy.arange(start, end)
    retx = self.center(i)
    rety = self.prob(i)
    
    return retx, rety


  def __iadd__(self, other):
    """Allows you to add the data collected in another RegOrogram into this one (+=). Will be efficient and without data loss if the spacing and block size match, otherwise it will switch to interpolating bin positions."""
    if numpy.fabs(self._spacing - other._spacing) < 1e-12 and self._blocksize==other._blocksize:
      # Efficient version...
      for k, v in other._data.items():
        if k in self._data:
          self._data[k] += v
        
        else:
          self._data[k] = v.copy()
    
    else:
      # Inefficient version - go via add() to get interpolation...
      indices = numpy.arange(other._blocksize)
      for k, v in other._data.items():
        x = other.center(k + indices)
        send = numpy.nonzero(v)
        self.add(x[send], v[send])
    
    # Update the stats...
    self._total += other._total
    self._low = min(self._low, other._low)
    self._high = max(self._high, other._high)
    
    self._cdf = None
  
  
  def __imul__(self, other):
    """Allows you to scale the weight of all of the collected samples by a positive real number; matters mostly as an operation before +=."""
    for v in other._data.values():
      v[:] *= other
    self._total *= other
  
  
  def _ensure_cdf(self):
    """Ensures that the _cdf array exists, creating it if needed"""
    if self._cdf is None:
      self._cdf = numpy.empty(self._high + 3 - self._low, dtype=numpy.float32)
      regorofun.cdf(self._data, self._blocksize, self._low, self._total, self._cdf)
      self._cdf[-1] = 1.0
  
  
  def cdf(self, x):
    """Evaluates the cdf at the given x, which can be a vector. Note that the cdf is a cached value, recalculated and invalidated as required, so alternating this with an operation such as add() would be very slow."""
    self._ensure_cdf()
    
    x = numpy.asarray(x) / self._spacing - (self._low - 1)
    base = numpy.floor(x).astype(int)
    t = x - base
    
    ret = (1 - t) * self._cdf[numpy.clip(base, 0,  self._cdf.shape[0]-1)]
    ret += t * self._cdf[numpy.clip(base+1, 0,  self._cdf.shape[0]-1)]
    return ret
  
  
  def bincdf(self, i):
    """Evaluates the cdf at the given bin center, which can be a vector. Note that the cdf is a cached value, recalculated and invalidated as required, so alternating this with an operation such as add() would be very slow."""
    self._ensure_cdf()
    
    return self._cdf[numpy.clip(i - (self._low - 1), 0,  self._cdf.shape[0]-1)]
  
  
  def cdfgraph(self, start = None, end = None):
    """Returns two aligned vectors, as a conveniance function for generating the arrays to hand to a graph plotting function to get the cdf. Return is a tuple of (x, y), x being the bin center and y the cdf. You can give the start and end bin if you want (inclusive, exclusive), but it defaults to the range of the observed data (going one wider so the end points are 0 and 1). Note that the cdf is a cached value, recalculated and invalidated as required, so alternating this with an operation such as add() would be very slow."""
    if start is None:
      start = self._low - 1
    
    if end is None:
      end = self._high + 2
    
    i = numpy.arange(start, end)
    retx = self.center(i)
    rety = self.bincdf(i)
    
    return retx, rety


  def draw(self, size = None, rng = None):
    """Draws samples from the distribution - first parameter is how many (defaults to None, which is one sample, not in an array; can be a tuple for a nD array), second something that numpy.random.default_rng() is happy to accept. Note that this uses the cdf (inverse cdf transform) which is a cached value, recalculated and invalidated as required, so alternating this with an operation such as add() would be very slow."""
    self._ensure_cdf()
    
    # Fetch the noise...
    rng = numpy.random.default_rng(rng)
    noise = rng.random(size, dtype=numpy.float32)
    
    # Do the inverse CDF dance...
    after = numpy.searchsorted(self._cdf, noise)
    t = (noise - self._cdf[after-1]) / (self._cdf[after] - self._cdf[after-1])
    
    x = ((after - 1 + t) + (self._low - 1)) * self._spacing
    return x


  def median(self):
    """Returns the median. Uses the cached cdf array, which is recalculated and invalidated as required, so alternating this with an operation such as add() would be very slow."""
    self._ensure_cdf()
    
    after = numpy.searchsorted(self._cdf, 0.5)
    t = (0.5 - self._cdf[after-1]) / (self._cdf[after] - self._cdf[after-1])
    
    return ((after - 1 + t) + (self._low - 1)) * self._spacing


  def mean(self):
    """Returns the mean. Calculation involves looping all data, so is not very efficient."""
    weight = 0.0
    mean = 0.0
    
    for k,v in self._data.items():
      # Block mean...
      bw = v.sum()
      m = (numpy.arange(self._blocksize) * v).sum() / bw
      bm = (k * self._blocksize + m) * self._spacing
      
      # Incrementally combine with mean thus far...
      weight += bw
      mean += (bm - mean) * bw / weight
    
    return mean
  
  
  def var(self):
    """Returns the variance. Calculation involves looping all data, so is not very efficient."""
    return self.meanvar()[1]
  
  
  def meanvar(self):
    """Returns a tuple containing (mean, variance). Calculation involves looping all data, so is not very efficient, but definitely more efficient than calling mean() and var() seperately."""
    weight = 0.0
    mean = 0.0
    scatter = 0.0
    
    for k,v in self._data.items():
      # Block weight, mean and scatter (variance * weight)...
      bw = v.sum()
      
      pos = numpy.arange(self._blocksize)
      m = (pos * v).sum() / bw
      bm = (k * self._blocksize + m) * self._spacing
      
      bs = (numpy.square(pos - m) * v).sum() * numpy.square(self._spacing)
      
      # Incrementally combine into parameters thus far...
      old_weight = weight
      weight += bw
      delta = bm - mean
      mean += delta * bw / weight
      scatter += bs + delta * delta * (old_weight * bw) / weight
    
    return mean, scatter / weight

  
  def entropy(self):
    """Calculates the entropy analytically, in nats."""
    # Evaluate probabilities...
    i = numpy.arange(self._low - 1, self._high + 2)
    prob = self.prob(i)
    
    # Entropy!..
    return regorofun.reg_entropy(prob, self._spacing)
  
  
  def entropynumint(self, samples=1024*1024, threshold=1e-12):
    """Numerical integration version of entropy(); for testing only as obviously much slower. In nats."""
    
    # Evaluate across range of distribution...
    x = numpy.linspace(self.center(self._low - 1), self.center(self._high + 2), samples)
    y = self(x)
    delta = x[1] - x[0]
    
    # Filter out the zeros - log gets very unhappy about them...
    y = y[y>=threshold]
    
    # Entropy!..
    return -delta * (y * numpy.log(y)).sum()
  
  
  def entropymc(self, samples=1024*1024, threshold=1e-12, rng=None):
    """Monte-Carlo integration version of entropy. Super slow of course so just for testing as it also doubles as a good sanity check of sampling. In nats."""
    
    # Draw and evaluate...
    x = self.draw(samples, rng)
    y = self(x)
    
    # Entropy!..
    return -numpy.log(numpy.maximum(y, 1e-12)).mean()
  
  
  def crossentropy(self, q):
    """Calculates the cross entropy, H(p=self, q=first parameter), outputing nats. If you're measuring the inefficency of an encoding then p/self is the true distribution and q/first parameter the distribution used for encoding."""
    if numpy.fabs(self._spacing - q._spacing) < 1e-12 and self._blocksize==q._blocksize:
      # Efficient version - the change points are aligned...
      return regorofun.aligned_crossentropy(self._blocksize, self._spacing, self._low-1, self._high+2, self._data, q._data, self._total, q._total)
    
    else:
      # Inefficient version - the change points are not aligned...
      return regorofun.misaligned_crossentropy(self._low-1, self._high+2, self._data, q._data, self._total, q._total, self._blocksize, q._blocksize, self._spacing, q._spacing)


  def crossentropynumint(self, q, samples=1024*1024, threshold=1e-12):
    """Calculates the cross entropy, H(p=self, q=first parameter), outputing nats. If you're measuring the inefficency of an encoding then p/self is the true distribution and q/first parameter the distribution used for encoding. This version uses numerical integration and exists for testing only - slow."""
    
    # Evaluate p across range of self distribution...
    x = numpy.linspace(self.center(self._low - 1), self.center(self._high + 2), samples)
    p = self(x)
    delta = x[1] - x[0]
    
    # Filter out the zeros - log gets very unhappy about them...
    good = p>=threshold
    x = x[good]
    p = p[good]
    
    # Evaluate log(q)... 
    qlog = numpy.log(numpy.maximum(q(x), 1e-32))
    
    # Crossentropy!..
    return -delta * (p * qlog).sum()


  def crossentropymc(self, q, samples=1024*1024, threshold=1e-12, rng=None):
    """Calculates the cross entropy, H(p=self, q=first parameter), outputing nats. If you're measuring the inefficency of an encoding then p/self is the true distribution and q/first parameter the distribution used for encoding. This version uses monte-carlo itnergation and exists for testing only - super slow."""
    
    # Draw and evaluate...
    x = self.draw(samples, rng)
    y = q(x)
    
    # Calculate cross-entropy...
    return -numpy.log(numpy.maximum(y, 1e-32)).mean()
  
  
  def kl(self, q):
    """Calculates the Kullback—Leibler divergence between two distributions, i.e. the expected extra nats needed for encoding data with the distrbution represented by this object when the encoder is optimised for the distribution of q, the first parameter to this method. Convenience method that uses the cross-entropy and entropy methods."""
    return self.crossentropy(q) - self.entropy()
